{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dosyadan veri okundu...\n",
      "alt üst degerler saptandı...\n",
      "train test setleri ayrıldı. veri satır sayısı:  (1000, 12)\n",
      "Neural Network olusturuldu... Yapı:  (11, 30, 1)\n",
      "train basladı. epoch:  10000  L_rate 0.1  momentum:  0.5\n",
      "Iteration      2K - Error: 179.613280\n",
      "Iteration      4K - Error: 179.613675\n",
      "Iteration      6K - Error: 179.665786\n",
      "Iteration      8K - Error: 179.643414\n",
      "Iteration     10K - Error: 179.628467\n",
      "train tamamlandı...\n",
      "test başladı...\n",
      "oran:  0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#\n",
    "# Transfer functions\n",
    "#\n",
    "class TransferFunctions:\n",
    "    def relu(x, derivative=False):\n",
    "        if (derivative == True):\n",
    "            for i in range(0, len(x)):\n",
    "                for k in range(len(x[i])):\n",
    "                    if x[i][k] > 0:\n",
    "                        x[i][k] = 1\n",
    "                    else:\n",
    "                        x[i][k] = 0\n",
    "            return x\n",
    "        for i in range(0, len(x)):\n",
    "            for k in range(0, len(x[i])):\n",
    "                if x[i][k] > 0:\n",
    "                    pass  # do nothing since it would be effectively replacing x with x\n",
    "                else:\n",
    "                    x[i][k] = 0\n",
    "        return x\n",
    "\n",
    "    def sgm(x, Derivative=False):\n",
    "        if not Derivative:\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "        else:\n",
    "            #out = sgm(x)\n",
    "            out = 1.0 / (1.0 + np.exp(-x))\n",
    "            return out * (1.0 - out)\n",
    "    \n",
    "    def linear(x, Derivative=False):\n",
    "        if not Derivative:\n",
    "            return x\n",
    "        else:\n",
    "            return 1.0\n",
    "    \n",
    "    def gaussian(x, Derivative=False):\n",
    "        if not Derivative:\n",
    "            return np.exp(-x**2)\n",
    "        else:\n",
    "            return -2*x*np.exp(-x**2)\n",
    "    \n",
    "    def tanh(x, Derivative=False):\n",
    "        if not Derivative:\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            return 1.0 - np.tanh(x)**2\n",
    "    \n",
    "    def truncLinear(x, Derivative=False):\n",
    "        if not Derivative:\n",
    "            y = x.copy()\n",
    "            y[y < 0] = 0\n",
    "            return y\n",
    "        else:\n",
    "            return 1.0\n",
    "        \n",
    "#\n",
    "# Classes\n",
    "#\n",
    "class BackPropagationNetwork:\n",
    "    \"\"\"A back-propagation network\"\"\"\n",
    "    \n",
    "    #\n",
    "    # Class methods\n",
    "    #\n",
    "    def __init__(self, layerSize, layerFunctions=None):\n",
    "        \"\"\"Initialize the network\"\"\"\n",
    "        \n",
    "        self.layerCount = 0\n",
    "        self.shape = None\n",
    "        self.weights = []\n",
    "        self.tFuncs = []\n",
    "        \n",
    "        # Layer info\n",
    "        self.layerCount = len(layerSize) - 1\n",
    "        self.shape = layerSize\n",
    "        \n",
    "        if layerFunctions is None:\n",
    "            lFuncs = []\n",
    "            for i in range(self.layerCount):\n",
    "                if i == self.layerCount - 1:\n",
    "                    lFuncs.append(TransferFunctions.linear)\n",
    "                else:\n",
    "                    lFuncs.append(TransferFunctions.sgm)\n",
    "        else:\n",
    "            if len(layerSize) != len(layerFunctions):\n",
    "                raise ValueError(\"Incompatible list of transfer functions.\")\n",
    "            elif layerFunctions[0] is not None:\n",
    "                raise ValueError(\"Input layer cannot have a transfer function.\")\n",
    "            else:\n",
    "                lFuncs = layerFunctions[1:]\n",
    "        \n",
    "        self.tFuncs = lFuncs\n",
    "        \n",
    "        # Data from last Run\n",
    "        self._layerInput = []\n",
    "        self._layerOutput = []\n",
    "        self._previousWeightDelta = []\n",
    "        \n",
    "        # Create the weight arrays\n",
    "        for (l1,l2) in zip(layerSize[:-1], layerSize[1:]):\n",
    "            self.weights.append(np.random.normal(scale=0.01, size = (l2, l1+1)))\n",
    "            self._previousWeightDelta.append(np.zeros((l2, l1+1)))\n",
    "    \n",
    "    #\n",
    "    # Run method\n",
    "    #\n",
    "    def Run(self, input):\n",
    "        \"\"\"Run the network based on the input data\"\"\"\n",
    "        \n",
    "        lnCases = input.shape[0]\n",
    "        \n",
    "        # Clear out the previous intermediate value lists\n",
    "        self._layerInput = []\n",
    "        self._layerOutput = []\n",
    "        \n",
    "        # Run it!\n",
    "        for index in range(self.layerCount):\n",
    "            # Determine layer input\n",
    "            if index == 0:\n",
    "                layerInput = self.weights[0].dot(np.vstack([input.T, np.ones([1, lnCases])]))\n",
    "            else:\n",
    "                layerInput = self.weights[index].dot(np.vstack([self._layerOutput[-1], np.ones([1, lnCases])]))\n",
    "            \n",
    "            self._layerInput.append(layerInput)\n",
    "            self._layerOutput.append(self.tFuncs[index](layerInput))\n",
    "        \n",
    "        return self._layerOutput[-1].T\n",
    "                 \n",
    "    #\n",
    "    # TrainEpoch method\n",
    "    #\n",
    "    def TrainEpoch(self, input, target, trainingRate = 0.01, momentum = 0.7): #trainingRate = 0.02, momentum = 0.5\n",
    "        \"\"\"This method trains the network for one epoch\"\"\"\n",
    "        \n",
    "        delta = []\n",
    "        lnCases = input.shape[0]\n",
    "        \n",
    "        # First run the network\n",
    "        self.Run(input)\n",
    "        \n",
    "        # Calculate our deltas\n",
    "        for index in reversed(range(self.layerCount)):\n",
    "            if index == self.layerCount - 1:\n",
    "                # Compare to the target values\n",
    "                output_delta = self._layerOutput[index] - target.T\n",
    "                error = np.sum(output_delta**2)\n",
    "                delta.append(output_delta * self.tFuncs[index](self._layerInput[index], True))\n",
    "            else:\n",
    "                # Compare to the following layer's delta\n",
    "                delta_pullback = self.weights[index + 1].T.dot(delta[-1])\n",
    "                delta.append(delta_pullback[:-1, :] * self.tFuncs[index](self._layerInput[index], True))\n",
    "            \n",
    "        # Compute weight deltas\n",
    "        for index in range(self.layerCount):\n",
    "            delta_index = self.layerCount - 1 - index\n",
    "            \n",
    "            if index == 0:\n",
    "                layerOutput = np.vstack([input.T, np.ones([1, lnCases])])\n",
    "            else:\n",
    "                layerOutput = np.vstack([self._layerOutput[index - 1], np.ones([1, self._layerOutput[index - 1].shape[1]])])\n",
    "            \n",
    "            curWeightDelta = np.sum(\\\n",
    "                                 layerOutput[None,:,:].transpose(2, 0 ,1) * delta[delta_index][None,:,:].transpose(2, 1, 0)\\\n",
    "                                 , axis = 0)\n",
    "            \n",
    "            weightDelta = trainingRate * curWeightDelta + momentum * self._previousWeightDelta[index]\n",
    "            \n",
    "            self.weights[index] -= weightDelta\n",
    "            \n",
    "            self._previousWeightDelta[index] = weightDelta\n",
    "        \n",
    "        return error\n",
    "    \n",
    "#normalization function\n",
    "def norm(dizi):\n",
    "    #global Maxs, Mins\n",
    "    #Maxs = np.amax(dizi,axis=0)\n",
    "    #Mins = np.amin(dizi,axis=0)\n",
    "    A = []\n",
    "    for d in dizi:\n",
    "        a = []\n",
    "        for i in range(d.shape[0]):\n",
    "            new = (d[i]-Mins[i])/(Maxs[i]-Mins[i])\n",
    "            a.append(new)\n",
    "        A.append(a)\n",
    "    New = np.array(A)\n",
    "    return New\n",
    "    \n",
    "#sorgu yapma fonksiyonu\n",
    "def Sor(inp):\n",
    "    A = []\n",
    "    a = []\n",
    "    for i in range(inp.shape[0]):\n",
    "        new = (inp[i]-Mins[i])/(Maxs[i]-Mins[i])\n",
    "        a.append(new)\n",
    "    A.append(a)\n",
    "    New = np.array(A)\n",
    "    print(\"normalize edildi: \",New)\n",
    "    deger = bpn.Run(New)\n",
    "    return deger\n",
    "\n",
    "#\n",
    "# If run as a script, create a test object\n",
    "#\n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    #veri içeri alma\n",
    "    from numpy import genfromtxt \n",
    "    my_data = genfromtxt('winequality-white.csv', delimiter=';',skip_header=1)\n",
    "    print(\"dosyadan veri okundu...\")\n",
    "    #for normalizaiton\n",
    "    global Maxs, Mins\n",
    "    Maxs = np.amax(my_data,axis=0)\n",
    "    Mins = np.amin(my_data,axis=0) \n",
    "    print(\"alt üst degerler saptandı...\")\n",
    "    #print(Maxs,Mins)\n",
    "    \n",
    "    ayrim = 1000\n",
    "    train = my_data[:ayrim,:] #my_data[:1200,:]\n",
    "    test = my_data[ayrim:,:]\n",
    "    train_input = train[:,:11]\n",
    "    train_target= train[:,11:]\n",
    "    test_input = test[:,:11]\n",
    "    test_target = test[:, 11:]\n",
    "    print(\"train test setleri ayrıldı. veri satır sayısı: \",train.shape )\n",
    "    \n",
    "    learning_rate = 0.03\n",
    "    momentum = 0.5\n",
    "    \n",
    "    #lvTarget = norm(train_target)\n",
    "    lvTarget = train_target/10\n",
    "    lvInput = norm(train_input)\n",
    "    #katman sayısı kadar Transfer Fonksiyonu EKLE\n",
    "    lFuncs = [None, TransferFunctions.relu, TransferFunctions.sgm, TransferFunctions.sgm]\n",
    "    #Katman yapısını belirle\n",
    "    bpn = BackPropagationNetwork((11,15,8,1), lFuncs)\n",
    "    print(\"Neural Network olusturuldu... Yapı: \",bpn.shape)\n",
    "    #iterasyon sayısı ve istenilen hata oranı gir\n",
    "    lnMax = 10000 #50000\n",
    "    lnErr = 1e-1  #1e-6 0.1\n",
    "    print(\"train basladı. epoch: \",lnMax,\" L_rate\",learning_rate,\" momentum: \",momentum)\n",
    "    for i in range(lnMax+1):\n",
    "        err = bpn.TrainEpoch(lvInput, lvTarget,learning_rate, momentum)\n",
    "        if i % 2000 == 0 and i > 0: #5000\n",
    "            print(\"Iteration {0:6d}K - Error: {1:0.6f}\".format(int(i/1000), err))\n",
    "        if err <= lnErr:\n",
    "            print(\"İstenilen hata oranına ulaşıldı. Iter: {0}\".format(i))\n",
    "            break     \n",
    "    print(\"train tamamlandı...\")   \n",
    "        \n",
    "    # Display output\n",
    "    #lvOutput = bpn.Run(lvInput)\n",
    "    #for i in range(train_input.shape[0]):\n",
    "        #print(\"Input: {0} Output: {1} expect: {2}\".format(train_input[i], lvOutput[i],lvTarget[i]))\n",
    "     #   print(\"Output: {0} expect: {1}\".format(lvOutput[i],lvTarget[i]))\n",
    "#test safhası\n",
    "    print(\"test başladı...\")\n",
    "    test_input_norm = norm(test_input)\n",
    "    test_output = bpn.Run(test_input_norm)\n",
    "    dogru = 0\n",
    "    for i in range(test_output.shape[0]):\n",
    "        if round(test_output[i][0],1) == test_target[i][0]/10:\n",
    "            dogru = dogru + 1\n",
    "    print(\"oran: \",100*dogru/i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
